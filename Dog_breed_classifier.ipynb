{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and Training a Dog Breed Classifier with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kaggle API client expects this file to be in ~/.kaggle, so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating directory and changing the current working directory\n",
    "!mkdir dog_dataset\n",
    "%cd dog_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for dataset\n",
    "!kaggle datasets list -s dogbreedidfromcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading dataset and coming out of directory\n",
    "!kaggle datasets download catherinehorng/dogbreedidfromcomp\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzipping downloaded file and removing unusable file\n",
    "!unzip dog_dataset/dogbreedidfromcomp.zip -d dog_dataset\n",
    "!rm dog_dataset/dogbreedidfromcomp.zip\n",
    "!rm dog_dataset/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important library imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels.csv file and checking shape and records\n",
    "labels_all = pd.read_csv(\"dog_dataset/labels.csv\")\n",
    "print(labels_all.shape)\n",
    "labels_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of each breeds\n",
    "breeds_all = labels_all[\"breed\"]\n",
    "breed_counts = breeds_all.value_counts()\n",
    "breed_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting random 3 breeds of our choice (Limitation due to computation power)\n",
    "CLASS_NAMES = ['pomeranian','entlebucher','scottish_deerhound']\n",
    "labels = labels_all[(labels_all['breed'].isin(CLASS_NAMES))]\n",
    "labels = labels.reset_index()\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating numpy matrix with zeros\n",
    "X_data = np.zeros((len(labels), 224, 224, 3), dtype='float32')\n",
    "# One hot encoding\n",
    "Y_data = label_binarize(labels['breed'], classes = CLASS_NAMES)\n",
    "\n",
    "# Reading and converting image to numpy array and normalizing dataset\n",
    "for i in tqdm(range(len(labels))):\n",
    "    img = image.load_img('dog_dataset/train/%s.jpg' % labels['id'][i], target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    x = np.expand_dims(img.copy(), axis=0)\n",
    "    X_data[i] = x / 255.0\n",
    "    \n",
    "# Printing train image and one hot encode shape & size\n",
    "print('\\nTrain Images shape: ',X_data.shape,' size: {:,}'.format(X_data.size))\n",
    "print('One-hot encoded output shape: ',Y_data.shape,' size: {:,}'.format(Y_data.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu', input_shape = (224,224,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (7,7), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(64, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(len(CLASS_NAMES), activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set into training and testing data sets\n",
    "X_train_and_val, X_test, Y_train_and_val, Y_test = train_test_split(X_data, Y_data, test_size = 0.1)\n",
    "# Splitting the training data set into training and validation data sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_and_val, Y_train_and_val, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "                    validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['accuracy'], color='r')\n",
    "plt.plot(history.history['val_accuracy'], color='b')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the accuracy in terms of percentage\n",
    "Y_pred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting image to compare\n",
    "plt.imshow(X_test[1,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# Finding max value from predition list and comaparing original value vs predicted\n",
    "print(\"Originally : \",labels['breed'][np.argmax(Y_test[1])])\n",
    "print(\"Predicted : \",labels['breed'][np.argmax(Y_pred[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with downloading the dataset creating the model and finding out the predictions using the model. We can optimize different hyper parameters in order to tune this model for a higher accuracy. This model can be used to predict different breeds of dogs which can be further used by different NGO's working on saving animals and for educational purposes or maybe just for curious dog lovers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
